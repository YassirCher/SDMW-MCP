# SDMW-MCP

# Projet MCP - IntÃ©gration Spring Boot & Python
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Spring Boot](https://img.shields.io/badge/Spring%20Boot-3.x-brightgreen.svg)
![Python](https://img.shields.io/badge/Python-3.x-blue.svg)

Une implÃ©mentation complÃ¨te du protocole Model Context Protocol (MCP) comprenant un client Spring Boot et un serveur Python. Ce systÃ¨me dÃ©montre une fonctionnalitÃ© de chat basÃ©e sur l'IA avec intÃ©gration d'outils pour les informations boursiÃ¨res, les donnÃ©es d'entreprise, les opÃ©rations sur le systÃ¨me de fichiers et la gestion des employÃ©s.

**Auteur**: Manus AI

## Table des MatiÃ¨res
- [Introduction](#introduction)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture](#architecture)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Technologies UtilisÃ©es](#technologies-utilisÃ©es)
- [Structure du Projet](#structure-du-projet)
- [Configuration de DÃ©veloppement](#configuration-de-dÃ©veloppement)
- [Points d'AccÃ¨s API](#points-d'accÃ¨s-api)
- [Configuration](#configuration)
- [Performance & Surveillance](#performance--surveillance)
- [DÃ©pannage](#dÃ©pannage)
- [Captures d'Ã‰cran](#captures-d'Ã©cran)
- [Contribuer](#contribuer)
- [Licence](#licence)
- [Contact](#contact)
- [FAQ](#faq)
- [DÃ©mo](#dÃ©mo)
- [Feuille de Route](#feuille-de-route)




## Introduction

Ce projet implÃ©mente le Model Context Protocol (MCP) avec une architecture multi-tiers :
- **Client MCP Spring Boot**: GÃ¨re les requÃªtes de chat IA et l'intÃ©gration d'outils.
- **Serveur MCP Python**: Fournit divers outils et services.
- **Serveur MCP Spring Boot**: Services d'entreprise supplÃ©mentaires.

Le systÃ¨me dÃ©montre le raisonnement de l'IA avec l'utilisation d'outils, la gestion de la mÃ©moire et les capacitÃ©s de chat en temps rÃ©el.




## FonctionnalitÃ©s

### ğŸ¤– Interface de Chat IA
- Chat interactif avec l'assistant IA
- Visualisation du processus de rÃ©flexion en temps rÃ©el
- Conversations conscientes de la mÃ©moire
- IntÃ©gration d'outils pour des capacitÃ©s amÃ©liorÃ©es

### ğŸ› ï¸ Outils Disponibles
- **Informations BoursiÃ¨res**: RÃ©cupÃ©ration de donnÃ©es boursiÃ¨res en temps rÃ©el
- **DonnÃ©es d'Entreprise**: Recherche d'informations sur les entreprises
- **OpÃ©rations sur le SystÃ¨me de Fichiers**: CapacitÃ©s de gestion de fichiers
- **Informations sur les EmployÃ©s**: Gestion des donnÃ©es RH

### ğŸ’» FonctionnalitÃ©s Techniques
- Architecture API RESTful
- RequÃªtes cross-origin (CORS) activÃ©es
- Persistance de la mÃ©moire entre les sessions
- Gestion des erreurs et validation




## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spring Boot   â”‚    â”‚   Python MCP    â”‚
â”‚   MCP Client    â”‚â—„â”€â”€â–ºâ”‚   Server        â”‚
â”‚   (Port 8066)   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spring Boot   â”‚
â”‚   MCP Server    â”‚
â”‚   (Port 8899)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```




## Installation

### PrÃ©requis
- **Java 21** ou version ultÃ©rieure
- **Python 3.12** ou version ultÃ©rieure
- **Maven** (ou utiliser le wrapper inclus)
- **Ollama** (pour le service de modÃ¨le IA) - Installer depuis [ollama.ai](https://ollama.ai) avec modele llama3.1:8b le moins volumineux (4,9GB)

### Cloner le DÃ©pÃ´t
```bash
git clone https://github.com/YassirCher/SDMW-MCP.git
```

### Guide de DÃ©marrage Rapide

Pour une dÃ©monstration rapide, suivez ces Ã©tapes dans l'ordre :

#### 1. Configuration d'Ollama (ModÃ¨le IA)
```bash
# Installer et dÃ©marrer Ollama
ollama serve

# Dans un nouveau terminal, tÃ©lÃ©charger le modÃ¨le requis
ollama pull llama3.1:8b
```

#### 2. DÃ©marrer le Serveur MCP Python
```bash
cd mcp-server-python
# Activer l'environnement virtuel existant
.venv\Scripts\activate  # Sous Windows
# source .venv/bin/activate  # Sous macOS/Linux

python server.py
```

#### 3. DÃ©marrer le Client MCP Spring Boot
```bash
cd mcp-client
./mvnw spring-boot:run
```

#### 4. DÃ©marrer le Serveur MCP Spring Boot (Optionnel)
```bash
cd mcp-server
./mvnw spring-boot:run
```

### VÃ©rification
Une fois tous les services en cours d'exÃ©cution, vous devriez voir :
- âœ… Serveur MCP Python : `http://localhost:8899`
- âœ… Client MCP Spring Boot : `http://localhost:8066`
- âœ… ModÃ¨le IA Ollama : `http://localhost:10000`




## Utilisation

### AccÃ©der Ã  l'Application
1. **API Backend**: http://localhost:8066
2. **Swagger UI**: http://localhost:8066/swagger-ui.html 

### Flux de Chat Basique
1. Utilisez des requÃªtes suggÃ©rÃ©es ou tapez les vÃ´tres via l'API.
2. Observez le processus de rÃ©flexion et la rÃ©ponse de l'IA.
3. Explorez les diffÃ©rents outils et capacitÃ©s.

### Exemples de RequÃªtes
- `"Bonjour, comment allez-vous ?"` - Salutation basique
- `"Quelles entreprises connaissez-vous ?"` - Informations sur l'entreprise
- `"Parlez-moi de l'action Apple"` - Informations boursiÃ¨res
- `"Lister tous les employÃ©s"` - DonnÃ©es des employÃ©s
- `"Quels fichiers sont disponibles ?"` - OpÃ©rations sur le systÃ¨me de fichiers




## Technologies UtilisÃ©es

### Backend
- **Spring Boot 3.x** - Cadre principal
- **Spring AI** - IntÃ©gration de l'IA
- **Spring Data JPA** - OpÃ©rations de base de donnÃ©es
- **H2 Database** - Base de donnÃ©es en mÃ©moire
- **Maven** - Outil de construction
- **Java 21** - Langage de programmation

### Serveur Python
- **Python 3.12** - Langage de programmation
- **MCP SDK** - Protocole de Contexte de ModÃ¨le




## Structure du Projet

```
mcp-project-spring-python/
â”œâ”€â”€ README.md
â”œâ”€â”€ mcp-client
â”‚   â”œâ”€â”€ HELP.md
â”‚   â”œâ”€â”€ mvnw
â”‚   â”œâ”€â”€ mvnw.cmd
â”‚   â”œâ”€â”€ pom.xml
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ main
â”‚   â”‚   â””â”€â”€ test
â”‚   â””â”€â”€ target
â”‚       â”œâ”€â”€ classes
â”‚       â”œâ”€â”€ generated-sources
â”‚       â”œâ”€â”€ generated-test-sources
â”‚       â””â”€â”€ test-classes
â”œâ”€â”€ mcp-server
â”‚   â”œâ”€â”€ HELP.md
â”‚   â”œâ”€â”€ mvnw
â”‚   â”œâ”€â”€ mvnw.cmd
â”‚   â”œâ”€â”€ pom.xml
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ main
â”‚   â”‚   â””â”€â”€ test
â”‚   â””â”€â”€ target
â”‚       â”œâ”€â”€ classes
â”‚       â”œâ”€â”€ generated-sources
â”‚       â”œâ”€â”€ generated-test-sources
â”‚       â””â”€â”€ test-classes
â”œâ”€â”€ pom.xml
â”œâ”€â”€ project_structure.txt
â”œâ”€â”€ python-mcp-server
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â””â”€â”€ server.cpython-312.pyc
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ python-mcp-server.iml
â”‚   â”œâ”€â”€ server.py
â”‚   â””â”€â”€ uv.lock
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ main
â”‚   â”‚   â”œâ”€â”€ java
â”‚   â”‚   â””â”€â”€ resources
â”‚   â””â”€â”€ test
â”‚       â””â”€â”€ java
â””â”€â”€ yassir-mcp-demo-spring-python.iml

26 directories, 19 files
```




## Configuration de DÃ©veloppement

### Fichiers de Configuration

#### PropriÃ©tÃ©s de l'Application (Spring Boot)
- **Port**: 8066
- **URL du Serveur MCP**: http://localhost:8899
- **Base de DonnÃ©es**: H2 en mÃ©moire
- **ModÃ¨le IA**: llama3.1 via Ollama

### Variables d'Environnement
```bash
# Optionnel : DÃ©finir les clÃ©s API si vous utilisez des services externes
CLAUDE_API_KEY=votre_clÃ©_api_ici
OLLAMA_BASE_URL=http://localhost:10000
```




## Points d'AccÃ¨s API

### Point d'AccÃ¨s Principal du Chat
- **GET** `/chat?query={message}` - Envoyer un message de chat Ã  l'IA

### Points d'AccÃ¨s des Outils (via le Serveur MCP)
- Outils d'information boursiÃ¨re
- RÃ©cupÃ©ration de donnÃ©es d'entreprise
- OpÃ©rations sur le systÃ¨me de fichiers
- Gestion des employÃ©s




## Configuration

### Fichiers de Configuration ClÃ©s

#### mcp-servers.json
Fichier de configuration pour les connexions au serveur MCP :
```json
{
  "servers": {
    "server1": {
      "url": "http://localhost:8899",
      "sse-endpoint": "/sse"
    }
  }
}
```

#### application.properties
ParamÃ¨tres de configuration clÃ©s :
```properties
# Configuration du Serveur
server.port=8066

# Configuration du Client MCP
spring.ai.mcp.client.type=sync
spring.ai.mcp.client.sse.connections.server1.url=http://localhost:8899
spring.ai.mcp.client.sse.connections.server1.sse-endpoint=/sse

# Configuration du ModÃ¨le IA
spring.ai.ollama.base-url=http://localhost:10000
spring.ai.ollama.chat.model=llama3.1

# Configuration de la Base de DonnÃ©es
spring.datasource.url=jdbc:h2:mem:testdb
spring.h2.console.enabled=true
```




## Performance & Surveillance

### Exigences du SystÃ¨me
- **MÃ©moire**: 16 Go de RAM minimum, 32 Go recommandÃ©s. Notez que les modÃ¨les de langage volumineux comme Llama 3.1:8B peuvent nÃ©cessiter 16 Go de RAM supplÃ©mentaires ou plus, en fonction de la taille du modÃ¨le et de l'utilisation. Pour les systÃ¨mes avec une RAM limitÃ©e, il est conseillÃ© d'utiliser des modÃ¨les plus petits comme `llama3.1` ou `mistral`.
- **CPU**: Processeur multi-cÅ“ur recommandÃ©
- **Stockage**: 8 Go d'espace libre pour les modÃ¨les et les dÃ©pendances
- **RÃ©seau**: Connexion Internet stable pour la configuration initiale

### MÃ©triques de Performance
- **Temps de DÃ©marrage**: ~2-3 minutes pour tous les services
- **Temps de RÃ©ponse**: <2 secondes pour les requÃªtes typiques
- **Utilisation de la MÃ©moire**: ~14 Go de RAM au total pour tous les services
- **DÃ©bit**: ~10 requÃªtes concurrentes prises en charge

### VÃ©rifications de SantÃ©
Surveiller la santÃ© du service Ã  :
- Spring Boot Actuator: `http://localhost:8066/actuator/health`
- Serveur MCP Python: VÃ©rifier la sortie de la console pour "Server listening on port 8899"




## DÃ©pannage

### ProblÃ¨mes Courants

#### Conflits de Ports
```bash
# VÃ©rifier si les ports sont utilisÃ©s
netstat -an | findstr "8066 8899 10000"

# Tuer les processus utilisant des ports spÃ©cifiques (Windows)
taskkill /F /PID <process_id>
```

#### ProblÃ¨mes de Version Java
```bash
# VÃ©rifier la version de Java
java -version

# Devrait afficher Java 21 ou version ultÃ©rieure
# Sinon, mettre Ã  jour la variable d'environnement JAVA_HOME
```

#### Ollama ne RÃ©pond pas
```bash
# VÃ©rifier si Ollama est en cours d'exÃ©cution
ollama list

# RedÃ©marrer le service Ollama
ollama serve

# TÃ©lÃ©charger le modÃ¨le requis
ollama pull llama3.1
```

#### ProblÃ¨mes d'Environnement Virtuel Python
```bash
# Sous Windows
cd mcp-server-python
.venv\Scripts\activate

# Sous macOS/Linux
cd mcp-server-python
source .venv/bin/activate

# RÃ©installer les dÃ©pendances
pip install -r requirements.txt
```

#### ProblÃ¨mes CORS
Si vous rencontrez des erreurs CORS :
1. Assurez-vous que le client MCP Spring Boot est en cours d'exÃ©cution sur le port 8066
2. VÃ©rifiez que `@CrossOrigin` est correctement configurÃ©
3. RedÃ©marrez l'application Spring Boot

### Mode DÃ©bogage
Activer la journalisation de dÃ©bogage dans `application.properties`:
```properties
logging.level.ma.tajeddine.mcpclient=DEBUG
logging.level.org.springframework.ai=DEBUG
```




## FAQ

### Questions GÃ©nÃ©rales

**Q: Puis-je utiliser diffÃ©rents modÃ¨les d'IA ?**
R: Oui, modifiez le fichier `application.properties` pour utiliser diffÃ©rents modÃ¨les Ollama :
```properties
spring.ai.ollama.chat.model=llama2
# ou
spring.ai.ollama.chat.model=mistral
```

**Q: Comment ajouter de nouveaux outils au serveur MCP ?**
R: Ã‰tendez le serveur MCP Python en :
1. Ajoutant de nouvelles dÃ©finitions d'outils dans `server.py`
2. ImplÃ©mentant la logique de l'outil dans des modules sÃ©parÃ©s
3. Enregistrant les outils auprÃ¨s du serveur MCP

**Q: Puis-je dÃ©ployer cela en production ?**
R: Oui, mais considÃ©rez :
- Utiliser des bases de donnÃ©es externes au lieu de H2
- Configurer une sÃ©curitÃ© appropriÃ©e (authentification, autorisation)
- Mettre en place un proxy inverse (nginx, Apache)
- Utiliser des variables d'environnement pour la configuration sensible

**Q: Comment sauvegarder l'historique du chat ?**
R: Actuellement, l'historique du chat est stockÃ© en mÃ©moire. Pour la persistance :
- ImplÃ©mentez le stockage en base de donnÃ©es dans le client Spring Boot
- Ajoutez des points d'accÃ¨s pour l'historique des conversations
- Envisagez d'utiliser Redis pour le stockage de session

### Questions Techniques

**Q: Pourquoi la rÃ©ponse de l'IA est-elle lente ?**
R: Causes possibles :
- Temps de chargement du modÃ¨le Ollama (premiÃ¨re requÃªte)
- Latence rÃ©seau vers le serveur MCP
- OpÃ©rations d'outils complexes
- Ressources systÃ¨me insuffisantes

**Q: Puis-je intÃ©grer avec Claude ou GPT au lieu d'Ollama ?**
R: Oui, dÃ©commentez et configurez dans `application.properties`:
```properties
spring.ai.anthropic.api-key=${CLAUDE_API_KEY}
spring.ai.anthropic.chat.options.model=claude-sonnet-4-20250514
```




## DÃ©mo

### Ã‰tapes de DÃ©monstration Rapide
1. DÃ©marrez tous les services en suivant le Guide de DÃ©marrage Rapide.
2. Utilisez un client API (comme Postman ou Swagger UI) pour interagir avec l'API Spring Boot sur `http://localhost:8066`.
3. Essayez ces exemples de requÃªtes :
   - "Bonjour, quel est votre nom ?"
   - "Lister toutes les entreprises"
   - "Parlez-moi d'Apple"
   - "Ã€ quels fichiers pouvez-vous accÃ©der ?"
   - "Retenez que mon nom est John"




## Captures d'Ã‰cran

Cette section prÃ©sente les diffÃ©rentes fonctionnalitÃ©s et capacitÃ©s du projet MCP Ã  travers des dÃ©monstrations visuelles.

<img width="1920" height="1080" alt="4" src="https://github.com/user-attachments/assets/1a10156e-c0e5-4450-9e7a-0f3804c7004e" />



Lien du Projet: [https://github.com/votre_utilisateur/mcp-project-spring-python](https://github.com/votre_utilisateur/mcp-project-spring-python)




## Feuille de Route

### FonctionnalitÃ©s PrÃ©vues
- [ ] Chat en temps rÃ©el avec support WebSocket
- [ ] Authentification et autorisation des utilisateurs
- [ ] Persistance de l'historique des conversations
- [ ] IntÃ©grations d'outils supplÃ©mentaires
- [ ] Conteneurisation Docker
- [ ] Manifestes de dÃ©ploiement Kubernetes
- [ ] Tests d'intÃ©gration
- [ ] Benchmarks de performance

### IdÃ©es de Contribution
- Ajouter de nouveaux outils MCP (mÃ©tÃ©o, actualitÃ©s, etc.)
- AmÃ©liorer la conception de l'interface utilisateur
- Ajouter des capacitÃ©s de chat vocal
- ImplÃ©menter des salons/canaux de discussion
- Ajouter des fonctionnalitÃ©s d'exportation/importation

---

â­ **Mettez une Ã©toile Ã  ce dÃ©pÃ´t si vous le trouvez utile !**




## ExpÃ©riences avec les ModÃ¨les d'IA

Lors du dÃ©veloppement de ce projet, plusieurs modÃ¨les de langage ont Ã©tÃ© testÃ©s pour l'intÃ©gration avec Spring AI et le support des outils. Voici un rÃ©sumÃ© des observations :

### ModÃ¨les Gratuits
- **Deepseek et Mistral (versions gratuites)**: Ces modÃ¨les ont Ã©tÃ© testÃ©s pour leur accessibilitÃ©. Cependant, ils n'ont pas dÃ©montrÃ© un support adÃ©quat pour l'intÃ©gration des outils spÃ©cifiques de Spring AI. Cela signifie que les fonctionnalitÃ©s avancÃ©es nÃ©cessitant l'appel de fonctions ou d'outils dÃ©finis dans l'application Spring Boot n'Ã©taient pas pleinement opÃ©rationnelles avec ces modÃ¨les.

### ModÃ¨les Payants ou LimitÃ©s
### Limitations techniques
- **OpenAI et Grok**: Ces modÃ¨les offrent des capacitÃ©s avancÃ©es et un bon support des outils, mais ils ne sont pas gratuits. Leur utilisation implique des coÃ»ts, ce qui peut Ãªtre un facteur limitant pour les projets personnels ou Ã  budget restreint.
- **Gemini (via Vertex AI)**: Bien que Gemini soit un modÃ¨le puissant et que des versions gratuites existent pour un usage gÃ©nÃ©ral, l'accÃ¨s aux modÃ¨les Gemini optimisÃ©s pour les agents et l'intÃ©gration d'outils (nÃ©cessaires pour ce type de projet) se fait souvent via des services cloud comme Google Cloud Vertex AI. Ces services ne sont pas gratuits et entraÃ®nent des coÃ»ts d'utilisation, ce qui rend l'expÃ©rimentation et le dÃ©ploiement plus onÃ©reux.
Le modÃ¨le `llama3.1:8b` est trÃ¨s volumineux et nÃ©cessite **au minimum 16GB de RAM** pour fonctionner correctement.
- Il requiert Ã©galement un **GPU performant** (idÃ©alement avec plus de 12GB de VRAM) pour un bon temps d'infÃ©rence.
- Lors de mes tests, un **problÃ¨me de mÃ©moire insuffisante** a Ã©tÃ© rencontrÃ© (voir capture ci-dessous).
- <img width="987" height="165" alt="image" src="https://github.com/user-attachments/assets/a2485f24-031e-491a-b225-364e1b837113" />
<img width="213" height="437" alt="image" src="https://github.com/user-attachments/assets/ebb6288c-05f0-4781-9233-11af9f89d147" />



Ces expÃ©riences soulignent l'importance de choisir un modÃ¨le d'IA non seulement en fonction de ses capacitÃ©s, mais aussi de ses exigences en ressources (RAM, GPU) et de son modÃ¨le Ã©conomique (gratuit, payant, freemium avec limitations).



